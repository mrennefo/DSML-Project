{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85dcb6d7-eabc-4f84-9b91-71c1fa1328f9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\"><h1 style=\"text-decoration: underline;\">DSML Project</h1></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e1d2bffc61bc85",
   "metadata": {},
   "source": [
    "This is the official Notebook of the DSML Project from Marc Rennefort, Kilian Lipinsky, Timo Hagelberg, Jan Behrendt-Emden and Paul Severin. In order to create this Project we used the following dataset: https://data.cityofchicago.org/Transportation/Transportation-Network-Providers-Trips-2023-2024-/n26f-ihde/about_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c90e750-76f6-4524-9e00-23d389391975",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T10:51:07.039492Z",
     "start_time": "2025-07-10T10:51:00.426734Z"
    }
   },
   "outputs": [],
   "source": [
    "#Note all your imports here\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from holoviews.plotting.plotly import ScatterPlot\n",
    "from mpmath import sumap\n",
    "from numpy.ma.core import inner\n",
    "from pandas.core.common import random_state\n",
    "#from skimage.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from meteostat import Hourly, Point\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad31350-3590-4527-b773-f4bd3984bfa9",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\"text-decoration: underline;\">1. Buisness Understanding</h4>\n",
    "The given dataset contains all trips in 2023-2024 to the city of chicago reportet by rideshare companies such as Uber. With this data we want to train a model that helps use to predict the tip in ride hailing trips as accurate as possible. This could be interesting for Uber drivers to plan their rides on routes and days which give the most tip. In addition it could also help Uber as a company to make appropriate sales forecasts. For our prediction we want to include the following features: travel time, distance, fare amount, weather conditions, and whether the customer shared the ride. At this point it should be said that the tip values are not exact because they are rounded to the nearest $1.00 and only digital tips are included in the dataset, as tips paid in cash don't get tracked.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb96b7050d8dc921",
   "metadata": {},
   "source": [
    "<h4 style=\"text-decoration: underline;\">2. Data Understanding and Data Prepertion</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90df07db112fe767",
   "metadata": {},
   "source": [
    "<h4 style=\"text-decoration: underline;\">2.1 Some Basic Data Preperation</h4>\n",
    "In the first step we want to do some basic data preperartion and data understanding which means that we load our data set with the columns we need, we drop all rows with null values and changing our timestamps to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76201386-ac55-4da8-ab2b-7d5fe6a50618",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T13:26:08.304632Z",
     "start_time": "2025-07-04T13:25:14.504897Z"
    }
   },
   "outputs": [],
   "source": [
    "#Loading our dataset with the columns we need\n",
    "data_cleaned = pd.read_csv('Data/Chicago_RideHailing_Data.csv', usecols= ['Trip End Timestamp', 'Trip Seconds', 'Trip Miles', 'Tip', 'Trip Total','Dropoff Centroid Latitude', 'Dropoff Centroid Longitude', 'Shared Trip Authorized', 'Shared Trip Match'])\n",
    "\n",
    "#We can't use the Total including Tip, so we will calculate the cost (Fare + other Charges) as Total - Tip\n",
    "data_cleaned['Cost'] = data_cleaned['Trip Total'] - data_cleaned['Tip']\n",
    "data_cleaned = data_cleaned.drop(columns=['Trip Total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28185221ab2587ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T13:26:11.161264Z",
     "start_time": "2025-07-04T13:26:08.321160Z"
    }
   },
   "outputs": [],
   "source": [
    "#Get some basic understanding of our data\n",
    "print('Null Values: ', data_cleaned.isnull().sum())\n",
    "data_cleaned.info()\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921cc3b-a9de-4994-83c6-e64c9cb4fb1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T13:26:15.353595Z",
     "start_time": "2025-07-04T13:26:11.338849Z"
    }
   },
   "outputs": [],
   "source": [
    "#Drop all rows with null values\n",
    "data_cleaned = data_cleaned.dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ba2335",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T13:27:35.279507Z",
     "start_time": "2025-07-04T13:26:15.400091Z"
    }
   },
   "outputs": [],
   "source": [
    "#Changing our timestamp to datetime format\n",
    "data_cleaned['Trip End Timestamp'] = pd.to_datetime(data_cleaned['Trip End Timestamp'],  format='%m/%d/%Y %I:%M:%S %p', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9270bcde-2698-40e4-9adf-315eb1f52d16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T13:27:37.062581Z",
     "start_time": "2025-07-04T13:27:35.287965Z"
    }
   },
   "outputs": [],
   "source": [
    "#Check if everything worked correctly\n",
    "print('Null-Werte: ', data_cleaned.isnull().sum())\n",
    "data_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8684bb9bbe0d210",
   "metadata": {},
   "source": [
    "<h4 style=\"text-decoration: underline;\">2.2 Including weather data</h4>\n",
    "In order to add the weather data we need to group our data because otherwise we will get runtime issues if we do API calls for barely 25 Million rows. This should be fine for our prediction purposes because there won't be huge differences in temperature or rain if we round by the second decimal place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f3c83a49f3c622",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T13:27:38.794591Z",
     "start_time": "2025-07-04T13:27:37.084720Z"
    }
   },
   "outputs": [],
   "source": [
    "#Round the Latitude and Longitude by the second decimal place and insert it in a new column\n",
    "data_cleaned[\"Latitude rounded\"] =  data_cleaned[\"Dropoff Centroid Latitude\"].round(2)\n",
    "data_cleaned[\"Longitude rounded\"] = data_cleaned[\"Dropoff Centroid Longitude\"].round(2)\n",
    "\n",
    "#Group the data by Latitude and Longitude\n",
    "data_grouped = data_cleaned.groupby([\"Latitude rounded\", \"Longitude rounded\"])[\"Trip End Timestamp\"].agg([\"min\", \"max\"]).reset_index()\n",
    "data_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e378e75201ad951f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T13:29:40.584564Z",
     "start_time": "2025-07-04T13:27:38.849746Z"
    }
   },
   "outputs": [],
   "source": [
    "weather_list = []\n",
    "for i in range(len(data_grouped)):\n",
    "    #Initalise variables\n",
    "    latitude = data_grouped[\"Latitude rounded\"].iloc[i]\n",
    "    longitude = data_grouped[\"Longitude rounded\"].iloc[i]\n",
    "    location = Point(latitude, longitude)\n",
    "    timestamp_min = data_grouped[\"min\"].iloc[i]\n",
    "    timestamp_max = data_grouped[\"max\"].iloc[i]\n",
    "    \n",
    "    #Round min and max column to the next hour in order to extract the weather data correctly\n",
    "    timestamp_min_rounded = timestamp_min.replace(minute = 0, second = 0) \n",
    "    timestamp_max_rounded = timestamp_max.replace(minute = 0, second = 0)\n",
    "    \n",
    "    #Extract the weather data per location\n",
    "    weather = Hourly(location, timestamp_min_rounded, timestamp_max_rounded).fetch()\n",
    "\n",
    "    #Merge the extracted weather data with the fitting timestamps and locations\n",
    "    for j in range(len(weather)):\n",
    "       weather_list.append({\"Timestamp\": weather.index[j], \"Latitude rounded\": latitude, \"Longitude rounded\": longitude, \"Temperature\": weather[\"temp\"].iloc[j], \"Rain in mm\": weather[\"prcp\"].iloc[j]})\n",
    "\n",
    "#Covert the list to a DataFrame\n",
    "weather_data = pd.DataFrame(weather_list)\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f673f12697bfe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T13:29:41.700664Z",
     "start_time": "2025-07-04T13:29:40.687756Z"
    }
   },
   "outputs": [],
   "source": [
    "#Now we prepare the merge of the weather data and the other data. For this we need to round our timestamps by the next hour because our weather data is given hourly\n",
    "data_cleaned[\"Trip End Timestamp Rounded\"] = data_cleaned[\"Trip End Timestamp\"].dt.floor(\"h\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84dfd98eb4e51ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T13:29:56.907116Z",
     "start_time": "2025-07-04T13:29:41.749763Z"
    }
   },
   "outputs": [],
   "source": [
    "#In the next step we can start with the merge\n",
    "data_merged = pd.merge(data_cleaned, weather_data_clean, left_on=[\"Trip End Timestamp Rounded\", \"Latitude rounded\", \"Longitude rounded\"], right_on =[\"Timestamp\", \"Latitude rounded\", \"Longitude rounded\"], how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae6dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we check if we introduced any null values with our weather data or during the merge\n",
    "print('Null-Werte: ', data_merged.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d93ac35cb9b957",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T13:29:59.201722Z",
     "start_time": "2025-07-04T13:29:57.156029Z"
    }
   },
   "outputs": [],
   "source": [
    "#After that we can drop all the columns we just needed to merge our data\n",
    "data_merged = data_merged.drop(columns = [\"Dropoff Centroid Latitude\", \"Dropoff Centroid Longitude\", \"Latitude rounded\", \"Longitude rounded\", \"Timestamp\" ,\"Trip End Timestamp Rounded\"])\n",
    "data_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6292de49063ad3e9",
   "metadata": {},
   "source": [
    "<h4 style=\"text-decoration: underline;\">2.3 Creation of dummy variables</h4>\n",
    "To perform or regression later on we need to transfer the 'Shared Trip Authorized', 'Shared Trip Match' and 'Trip End Timestamp' column to numeric datatype. For this we make use of dummy variable where 1 stands for true and 0 stands for false for the Shared Trip variables and different variables for the time of day in case of the Trip End variable. In addition we need to creat dummy variables for the rain because we are not interested in the amount of rain on a certain day rather we want to plot wheter it rained or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d11db014a286ddc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T13:30:00.890040Z",
     "start_time": "2025-07-04T13:29:59.338427Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create dummy variable for 'Shared Trip Authorized' and 'Shared Trip Match' (1 = True and 0 = False)\n",
    "data_merged[\"Shared Trip Authorized\"] = data_merged[\"Shared Trip Authorized\"].astype(int)\n",
    "data_merged[\"Shared Trip Match\"] = data_merged[\"Shared Trip Match\"].astype(int)\n",
    "print(\"üìãFirst 5 Rows:\")\n",
    "data_merged.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa41d72f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T13:30:05.787665Z",
     "start_time": "2025-07-04T13:30:02.471245Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create dummy variables for the Trip End Timestamp: 22:00-6:00 for night, 6:00-18:00 for day and 18:00-22:00 for evening\n",
    "data_merged[\"Trip End Hour\"] = data_merged[\"Trip End Timestamp\"].dt.hour\n",
    "data_merged[\"Night\"] = np.where((data_merged[\"Trip End Hour\"] >= 22) | (data_merged[\"Trip End Hour\"] < 6), 1, 0)\n",
    "data_merged[\"Day\"] = np.where((data_merged[\"Trip End Hour\"] >= 6) & (data_merged[\"Trip End Hour\"] < 18), 1, 0)\n",
    "data_merged[\"Evening\"] = np.where((data_merged[\"Trip End Hour\"] >= 18) & (data_merged[\"Trip End Hour\"] < 22), 1, 0)\n",
    "#Drop the Trip End Hour & Trip End Timestamp column as we don't need them anymore\n",
    "data_merged = data_merged.drop(columns=[\"Trip End Hour\", \"Trip End Timestamp\"])\n",
    "\n",
    "data_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0708826312343ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T13:30:09.969405Z",
     "start_time": "2025-07-04T13:30:05.887960Z"
    }
   },
   "outputs": [],
   "source": [
    "#We see that the column 'Rain in mm' is from datatype object but we need a numeric datatype, so we need to transform this column to the right datatype\n",
    "data_merged['Rain in mm'] = pd.to_numeric(data_merged['Rain in mm'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf622b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T13:30:10.688079Z",
     "start_time": "2025-07-04T13:30:10.040381Z"
    }
   },
   "outputs": [],
   "source": [
    "#Switching the columns so that all dummy variables will be after all numeric variables\n",
    "data_merged = data_merged[['Tip', 'Trip Seconds', 'Trip Miles', 'Cost', 'Temperature', 'Rain in mm', 'Shared Trip Authorized', 'Shared Trip Match', 'Day', 'Evening', 'Night']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c28864e3a57b2",
   "metadata": {},
   "source": [
    "<h4 style=\"text-decoration: underline;\">2.4 Dealing with outliers</h4>\n",
    "Outliers are often caused by errors in data collection. Therefore it is important to identify and remove them as they can distort the performance of predictive models by representing values that do not reflect typical or real-world scenarios. In the following we attempted to remove outliers using the standard deviation method. However, it must be acknowledged that we cannot say with complete certainty whether each detected outlier represents a data collection error or simply reflects rare but valid cases‚Äîsince we did not collect the data ourselves. Nevertheless this approach helps us prepare the dataset as effectively as possible for building robust predictive models\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5794063011b33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T13:30:14.086438Z",
     "start_time": "2025-07-04T13:30:10.701594Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#We don't need to deal with outliers on the columns with dummy variables because there can't be outliers if we just have the values 0 or 1\n",
    "data_merged_without_tip = data_merged.drop(columns = [\"Shared Trip Authorized\", \"Shared Trip Match\", \"Night\", \"Day\", \"Evening\"])\n",
    "for columns in data_merged_without_tip.columns:\n",
    "    #Calculate mean and standard deviation of the current column\n",
    "    mean = data_merged_without_tip[columns].mean()\n",
    "    std = data_merged_without_tip[columns].std()\n",
    "    \n",
    "    #Calculate upper and lower limit\n",
    "    upperlimit = mean + 3 * std\n",
    "    lowerlimit = mean - 3 * std\n",
    "\n",
    "    #Replace all outliers with null values so we can remove them later\n",
    "    data_merged.loc[(data_merged[columns] > upperlimit) | (data_merged[columns] < lowerlimit), columns] = np.nan\n",
    "\n",
    "#Remove all null values (delete all outliers)    \n",
    "data_without_outliers = data_merged.dropna(axis = 0)\n",
    "print(f\"‚ùå Deleted {len(data_merged) - len(data_without_outliers)} outliers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e10fae",
   "metadata": {},
   "source": [
    "<h4 style=\"text-decoration: underline;\">2.5 Saving the data</h4>\n",
    "We will save the preperated data here, so that we can continue working with it, without having to rerun all the code above every time we start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf3b4f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T09:06:02.178765Z",
     "start_time": "2025-07-08T09:06:02.109549Z"
    }
   },
   "outputs": [],
   "source": [
    "data_without_outliers.to_csv('Data/Chicago_RideHailing_Data_Cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4010aeb95861dbd",
   "metadata": {},
   "source": [
    "<h4 style=\"text-decoration: underline;\">3. Data Modeling</h4>\n",
    "If you have already saved the data you can use this shortcut to save some runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30556712",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T10:51:29.484724Z",
     "start_time": "2025-07-10T10:51:14.809577Z"
    }
   },
   "outputs": [],
   "source": [
    "data_without_outliers = pd.read_csv('Data/Chicago_RideHailing_Data_Cleaned.csv')\n",
    "#Check if everything worked correctly\n",
    "data_without_outliers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90765fe319b7467",
   "metadata": {},
   "source": [
    "<h4 style=\"text-decoration: underline;\">3.1 Train Test Validation Split</h4>\n",
    "First of all we need to split our data in train, test and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982ea1825479e2d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T10:51:44.705710Z",
     "start_time": "2025-07-10T10:51:29.617258Z"
    }
   },
   "outputs": [],
   "source": [
    "#Define x and y vectors\n",
    "x = data_without_outliers.drop(columns = [\"Tip\"])\n",
    "y = data_without_outliers[\"Tip\"]\n",
    "\n",
    "#Perform train test validation split\n",
    "x_train_data, x_test_data, y_train_data, y_test_data = train_test_split(x, y, test_size = 0.5, random_state = 42)\n",
    "x_val_data, x_test_data, y_val_data, y_test_data = train_test_split(x_test_data, y_test_data, test_size = 0.6, random_state = 42)\n",
    "print(\"Datasplit:\")\n",
    "print(f\"üèãÔ∏è Training: {len(x_train_data)} Samples ({len(x_train_data)/len(data_without_outliers)*100:.1f}%)\")\n",
    "print(f\"üî¨ Testing: {len(x_test_data)} Samples ({len(x_test_data)/len(data_without_outliers)*100:.1f}%)\")\n",
    "print(f\"‚úÖ Validation: {len(x_val_data)} Samples ({len(x_val_data)/len(data_without_outliers)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2af61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This second split will only be important for our second model\n",
    "\n",
    "x_train1_data, x_train2_data, y_train1_data, y_train2_data = train_test_split(x_train_data, y_train_data, test_size = 0.5, random_state = 42)\n",
    "print(\"Datasplit:\")\n",
    "print(f\"Train Model 1: {len(x_train1_data)} Samples ({len(x_train1_data)/len(x_train_data)*100:.1f}%)\")\n",
    "print(f\"Train Model 2: {len(x_train2_data)} Samples ({len(x_train2_data)/len(x_train_data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ba2b2a",
   "metadata": {},
   "source": [
    "<h4 style=\"text-decoration: underline;\">3.2 Descriptive analyses Split</h4>\n",
    "Now we will look at key statistics about our data. We only use the training data for this, in order to rule out accidental data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523acc14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T10:51:47.557796Z",
     "start_time": "2025-07-10T10:51:44.951393Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create a table with a column for each feature and key statistics as rows\n",
    "def create_statistics_table(df):\n",
    "    stats = pd.DataFrame(index=['Mean', 'Median', 'Standard Deviation', 'Min', 'Max', 'Range'])\n",
    "    for column in df.columns:\n",
    "        stats[column] = [\n",
    "            df[column].mean(),\n",
    "            df[column].median(),\n",
    "            df[column].std(),\n",
    "            df[column].min(),\n",
    "            df[column].max(),\n",
    "            df[column].max() - df[column].min()\n",
    "        ]\n",
    "    return stats\n",
    "\n",
    "#Create statistics table for the training data\n",
    "features_stats_table = create_statistics_table(x_train_data.drop(columns=['Shared Trip Authorized', 'Shared Trip Match', 'Night', 'Day', 'Evening']))\n",
    "target_stats_table = create_statistics_table(y_train_data.to_frame(name='Tip'))\n",
    "stats_table = pd.concat([target_stats_table.rename(columns={'Tip': 'Tip (Target)'}), features_stats_table], axis=1)\n",
    "stats_table.head(len(stats_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create a table presenting the share of 1s for each dummy variable in the training data\n",
    "dummy_share = pd.DataFrame({\n",
    "    'Shared Trip Authorized': x_train_data['Shared Trip Authorized'].value_counts(normalize=True),\n",
    "    'Shared Trip Match': x_train_data['Shared Trip Match'].value_counts(normalize=True),\n",
    "    'Night': x_train_data['Night'].value_counts(normalize=True),\n",
    "    'Day': x_train_data['Day'].value_counts(normalize=True),\n",
    "    'Evening': x_train_data['Evening'].value_counts(normalize=True)\n",
    "})\n",
    "\n",
    "dummy_share.head(len(dummy_share))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e6f7fdb19fb143",
   "metadata": {},
   "source": [
    "In the next step we plot the data to get an first overview on our predictors. For this we use the first 5000 values of each predictor because if we would use all value we cannot see any possible linear relationship because there would be too much data in one scatter plot. In addition it leads to a much shorter runtime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e774fd7e0d771cb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T10:51:49.825658Z",
     "start_time": "2025-07-10T10:51:47.637852Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create Scatterplots for the first 4 predictors\n",
    "fig_1, axes_1 = plt.subplots(nrows = 1, ncols = 4, figsize= (21,6))\n",
    "fig_1.suptitle(\"Scatterplots of all predictors\", fontsize=26)\n",
    "for i, ax in enumerate(axes_1):\n",
    "    ax.scatter(x = x_train_data.iloc[:5000, i], y = y_train_data[:5000], color = f'C{i}')\n",
    "    ax.set_title(x_train_data.columns[i])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Create Scatterplots for predictor 5-7\n",
    "fig_2, axes_2 = plt.subplots(nrows = 1, ncols = 3, figsize= (21,6))\n",
    "for i, ax in enumerate(axes_2):\n",
    "    x_values = x_train_data.iloc[:5000, i + 4]\n",
    "    y_values = y_train_data[:5000]\n",
    "    \n",
    "    # Kombiniere x und y Werte f√ºr das Z√§hlen\n",
    "    points_df = pd.DataFrame({'x': x_values, 'y': y_values})\n",
    "    \n",
    "    # Z√§hle die H√§ufigkeit jeder einzigartigen Kombination\n",
    "    point_counts = points_df.groupby(['x', 'y']).size().reset_index(name='count')\n",
    "    \n",
    "    # Erstelle die Gr√∂√üen basierend auf der Anzahl (skaliert f√ºr bessere Sichtbarkeit)\n",
    "    sizes = point_counts['count']\n",
    "    \n",
    "    ax.scatter(x = point_counts['x'], y = point_counts['y'], \n",
    "               s = sizes, color = f'C{i + 4}')\n",
    "    ax.set_title(x_train_data.columns[i + 4])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Create Scatterplots for predictor 8-10\n",
    "fig_3, axes_3 = plt.subplots(nrows = 1, ncols = 3, figsize= (21,6))\n",
    "for i, ax in enumerate(axes_3):\n",
    "    x_values = x_train_data.iloc[:5000, i + 7]\n",
    "    y_values = y_train_data[:5000]\n",
    "    \n",
    "    # Kombiniere x und y Werte f√ºr das Z√§hlen\n",
    "    points_df = pd.DataFrame({'x': x_values, 'y': y_values})\n",
    "    \n",
    "    # Z√§hle die H√§ufigkeit jeder einzigartigen Kombination\n",
    "    point_counts = points_df.groupby(['x', 'y']).size().reset_index(name='count')\n",
    "    \n",
    "    # Erstelle die Gr√∂√üen basierend auf der Anzahl (skaliert f√ºr bessere Sichtbarkeit)\n",
    "    sizes = point_counts['count']\n",
    "    \n",
    "    ax.scatter(x = point_counts['x'], y = point_counts['y'], \n",
    "               s = sizes, color = f'C{i + 7}')\n",
    "    ax.set_title(x_train_data.columns[i + 7])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a560e5dc95786d",
   "metadata": {},
   "source": [
    "<h4 style=\"text-decoration: underline;\">3.3 Data normalization</h4>\n",
    "After plotting the data we get some interesting information. It looks like there are a lot of linear correlations for example between the costs and the tip. Moreover it is noticeable that the predictors are on diffrent scales so we need to normalize them to get meaningful result in our regression later on. For the normalization we make use of the python libary StandartScaler which uses the following formula:\n",
    "\\[\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "\\] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227744ba4d1020f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T10:51:55.728223Z",
     "start_time": "2025-07-10T10:51:49.863635Z"
    }
   },
   "outputs": [],
   "source": [
    "#Select which Data to normalize (no Dummy Variables)\n",
    "columns_to_normalize = [\"Trip Seconds\", \"Trip Miles\", \"Cost\", \"Temperature\", \"Rain in mm\"]\n",
    "\n",
    "\n",
    "x_train_data_scaled = x_train_data.copy()\n",
    "x_val_data_scaled = x_val_data.copy()\n",
    "x_test_data_scaled = x_test_data.copy()\n",
    "x_train1_data_scaled = x_train1_data.copy()\n",
    "x_train2_data_scaled = x_train2_data.copy()\n",
    "\n",
    "\n",
    "#Normalize data\n",
    "scaler = StandardScaler()\n",
    "x_train_data_scaled[columns_to_normalize] = scaler.fit_transform(x_train_data[columns_to_normalize])\n",
    "#Hier nur noch transform verwenden, damit Mittelwert und Standardabweichung nicht neu berechnet werden\n",
    "x_val_data_scaled[columns_to_normalize] = scaler.transform(x_val_data[columns_to_normalize])\n",
    "x_test_data_scaled[columns_to_normalize] = scaler.transform(x_test_data[columns_to_normalize])\n",
    "x_train1_data_scaled[columns_to_normalize] = scaler.transform(x_train1_data[columns_to_normalize])\n",
    "x_train2_data_scaled[columns_to_normalize] = scaler.transform(x_train2_data[columns_to_normalize])\n",
    "\n",
    "\n",
    "x_train_data_scaled.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d74f7440394679",
   "metadata": {},
   "source": [
    "<h4 style=\"text-decoration: underline;\">3.4 Linear Regression</h4>\n",
    "\n",
    "After finishing the data preperation and completing our descripive task we now want to start with our predictive models. First of all we want to create an simple linear regression because we have seen a few linear relationships between our predictors and the tip in the descriptive task above. An linear regression is a good first attemp to create good understandable machine learning models because it is easy to implement and to interpret. Espacially in approximatiely lineare realtionships (as we have seen in our scatter plots before) this model is really robust and gives us good predictions. But in complex problems (where the realationships are not linear) this easy approach could be too simple and is not usable in order to represent reality. So this step is used to see if our prediction problem could be solved well by an easy understandable model or if it needs more complex regression tasks to create appropriate predictions.\n",
    " \n",
    "We define our linear regression model with the following input vector:\n",
    "\\begin{equation}\n",
    "x^{(i)} = \\left[ \\begin{array}{c} 1 \\\\ \\mathrm{Trip\\ Seconds}^{(i)} \\\\ \\mathrm{Trip\\ Miles}^{(i)} \\\\ \\mathrm{Shared\\ Trip\\ Authorized}^{(i)} \\\\ \\mathrm{Shared\\ Trip\\ Matched}^{(i)} \\\\ \\mathrm{Cost}^{(i)} \\\\ \\mathrm{Temperature}^{(i)} \\\\ \\mathrm{Rain\\ in\\ mm}^{(i)} \\\\ \\mathrm{Night}^{(i)} \\\\ \\mathrm{Day}^{(i)} \\\\\n",
    "\\mathrm{Evening}^{(i)} \\end{array} \\right] \\end{equation}\n",
    "\n",
    "In addition we create our linear hypothesis function $tip_\\beta(x) = \\beta^Tx$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "tip_\\beta(x) = \\beta_0 + \\beta_1 * \\mathrm{Trip\\ Seconds}^{(i)} + \\beta_2 * \\mathrm{Trip\\ Miles}^{(i)} + \\beta_3 * \\mathrm{Shared\\ Trip\\ Authorized}^{(i)} + \\\\ \\beta_4 * \\mathrm{Shared\\ Trip\\ Matched}^{(i)} + \\beta_5 * \\mathrm{Cost}^{(i)} + \\beta_6 * \\mathrm{Temperature}^{(i)} + \\beta_7 * \\mathrm{Rain\\ In\\ MM}^{(i)} + \\\\ \\beta_8 * \\mathrm{Night}^{(i)} + \\beta_9 * \\mathrm{Day}^{(i)} + \\beta_{10} * \\mathrm{Evening}^{(i)} \n",
    "\\end{equation}\n",
    "\n",
    "After defining our linear regression model, we now want to find the right values for our predictors $ \\beta_0, \\beta_1, \\dots, \\beta_9 $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6705753cb1b7f98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T10:52:01.102285Z",
     "start_time": "2025-07-10T10:51:55.788334Z"
    }
   },
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()\n",
    "linear_model.fit(x_train_data_scaled, y_train_data)\n",
    "print(\"üìãValues for our predictors:\")\n",
    "print(str(linear_model.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695ea9bf6e0ab608",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T10:52:01.605786Z",
     "start_time": "2025-07-10T10:52:01.171338Z"
    }
   },
   "outputs": [],
   "source": [
    "linear_model_prediction = linear_model.predict(x_val_data_scaled)\n",
    "print(\"‚ùåMean absoulte Error:\", round(mean_absolute_error(y_val_data, linear_model_prediction), 4), \"\\nü§ñR¬≤:\", round(r2_score(y_val_data, linear_model_prediction), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1092c3518ebfaf04",
   "metadata": {},
   "source": [
    "As we can see, our model has a low R¬≤ value and a high error which indicates that the predictions from our linear model differ significantly from the actual values. We can visualize this by creating a scatter plot of the predicted tips versus the actual tip amounts and adding an ideal line that represents perfect predictions. This allows us to see how far off our model's predictions are from the ideal.\n",
    "\n",
    "It becomes especially apparent that the model struggles with higher tip amounts (greater than $4) since it never predicts values above that threshold. This is probablly caused by the large number of zero values in our dataset, which lowers the slope of the regression line and makes the model systematically underestimate higher tips.\n",
    "\n",
    "Another possible reason why our predtictions differ a lot from the actual tip is that the tips are rounded by the nearest 1.00(see buisness understanding above) which cannot be well represented by our linear model. Because the tip values are rounded to full euros we suggest that a classification approach will likely work better than regression. The large gaps between the data points make it difficult to fit a precise regression line. Instead it seems more effective to divide the data into 10 classes (from 0 to \\$9) and predict one of these classes using classification algorithms\n",
    "\n",
    "In conclusion it becomes visible that our simple regression model is not appropriate to depict the real world and we need to try out other (classifiction) approaches to create a better and more robust model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a860faa0925d44f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T10:52:02.148908Z",
     "start_time": "2025-07-10T10:52:01.632171Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "#Plot ideal line\n",
    "plt.plot([y_val_data.min(), y_val_data.max()], [y_val_data.min(), y_val_data.max()], 'r--')\n",
    "#Scatter plot prediction versus actual tip\n",
    "plt.scatter(prediction[:5000], y_val_data[:5000])\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Actual Tip')\n",
    "plt.title('Actual versus predicted Tip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1b77335c6e83c4",
   "metadata": {},
   "source": [
    "<h4 style=\"text-decoration: underline;\">3.5 Decision Tree</h4>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9158ca46",
   "metadata": {},
   "source": [
    "Versuch mit Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d89d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechne den Anteil der Reihen in der Test Data, wo \"Tip\"=0 ist\n",
    "zero_tip_count = (y_test_data == 0).sum()\n",
    "total_count = len(y_test_data)\n",
    "zero_tip_percentage = (zero_tip_count / total_count) * 100\n",
    "print(f\"üìä Der Anteil der Reihen in der Test Data, wo 'Tip' = 0 ist: {zero_tip_percentage:.1f}%\")\n",
    "print(zero_tip_count, \"Reihen haben 'Tip' = 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b0ffd6eb4f2aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T13:19:49.212933Z",
     "start_time": "2025-07-10T13:19:49.164652Z"
    }
   },
   "outputs": [],
   "source": [
    "def tree_performance_test(predictions, y_data, isBinary=False):\n",
    "    print(f\"Accuracy: {accuracy_score(y_data, predictions):.4f}\")\n",
    "    if isBinary:\n",
    "        print(f\"Recall (Anteil 1er, die erkannt wurden): {recall_score(y_data, predictions):.4f}\")\n",
    "        print(f\"Precision (Anteil 1er-Vorhersagen, die Richtig waren): {precision_score(y_data, predictions):.4f}\")\n",
    "        print(f\"F1-Score: {f1_score(y_data, predictions):.4f}\")\n",
    "        print(\"---\")\n",
    "    else:\n",
    "        print(f\"Recall (Durchschnittlicher Anteil wahrer Vorhersagen an allen Vorhersagen pro Klasse): {recall_score(y_data, predictions, average='macro'):.4f}\")\n",
    "        print(f\"Precision (Durchschnittlicher Anteil an Werten pro Klasse, die vorhergesagt wurden): {precision_score(y_data, predictions, average='macro'):.4f}\")\n",
    "        print(f\"F1-Score: {f1_score(y_data, predictions, average='macro'):.4f}\")\n",
    "    non_zero_predictions = (predictions != 0).sum()\n",
    "    zero_predictions = (predictions == 0).sum()\n",
    "    print(f\"Anzahl der nicht-0-Vorhersagen: {non_zero_predictions}\")\n",
    "    print(f\"Anzahl der 0-Vorhersagen: {zero_predictions}\")\n",
    "    print()\n",
    "\n",
    "def show_correctness(predictions, y_val_data, max=None):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    right_counts = []\n",
    "    sum = []\n",
    "    for i in range(int(y_val_data.max()) + 1):\n",
    "        right_predictions = ((predictions == i) & (y_val_data == i)).sum()\n",
    "        sum_predictions = (y_val_data == i).sum()\n",
    "        right_counts.append(right_predictions)\n",
    "        sum.append(sum_predictions)\n",
    "    tips = range(int(y_val_data.max()) + 1) \n",
    "    plt.bar(tips, sum, label = \"Total Tips\")\n",
    "    plt.bar(tips, right_counts, label = \"Correct predicted\")\n",
    "    plt.ylabel(\"Total Tips in million\")\n",
    "    plt.xlabel(\"Tip amounts\")\n",
    "    plt.legend()\n",
    "    if max is not None:\n",
    "        plt.ylim(0, max)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054af71f4ac6d9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T10:52:22.551429Z",
     "start_time": "2025-07-10T10:52:02.341371Z"
    }
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=5, random_state = 42)\n",
    "model.fit(x_train_data_scaled, y_train_data)\n",
    "predictions = model.predict(x_val_data_scaled)\n",
    "tree_performance_test(predictions, y_val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2560f20",
   "metadata": {},
   "source": [
    "As we can see, due to our high share of 0 in the tips (75%), a simple tree will always predict 0, as that prediction has by far the highest chance of being correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f9ca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=60, random_state = 42)\n",
    "model.fit(x_train_data_scaled, y_train_data)\n",
    "predictions = model.predict(x_val_data_scaled)\n",
    "tree_performance_test(predictions, y_val_data)\n",
    "show_correctness(predictions, y_val_data)\n",
    "show_correctness(predictions, y_val_data, max=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c54ef0",
   "metadata": {},
   "source": [
    "So a more complex tree does predict other values, but for any tip thats not 0 it doesn't get it right very often.\n",
    "\n",
    "Now our idea is, to first predicts if a customer tips at all (regardless of the amount) and predict the amount in a second step. Our hope is, that by only creating two categories, our model does have a 75% chance of hitting no tip and a 25% chance of hitting a ride with tip, instead of a a way lower chance for the individual amount. After we find a lot of rides without tips, we think that a prediction of the tip amount will work way better, because the class of tip=0 will not be as dominant anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2418dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train1_binary = (y_train1_data != 0).astype(int)  # 1 wenn Y != 0, sonst 0\n",
    "y_val_binary = (y_val_data != 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca52cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "binary_model = RandomForestClassifier(\n",
    "    n_estimators=5,\n",
    "    max_depth=17, \n",
    "    random_state=42, \n",
    "    class_weight={0: 1, 1: 3.5} # Nicht-0 h√∂her gewichtet, um die Dominanz der 0er zu verringern\n",
    ")\n",
    "\n",
    "binary_model.fit(x_train1_data_scaled, y_train1_binary)\n",
    "binary_predictions = binary_model.predict(x_val_data_scaled)\n",
    "\n",
    "tree_performance_test(binary_predictions, y_val_binary, isBinary=True)\n",
    "show_correctness(binary_predictions, y_val_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d28893",
   "metadata": {},
   "source": [
    "This model identifies 72% of rides with tips, which means that 28% of rides with tips will be predicted with no tip. This is the best we could achieve.\n",
    "\n",
    "For the rides where the model predicts a tip, we still have to predict how much will be tipped. Our hope of having a dataset with a less dominant class of 0 did not get fulfilled (now we have 70% of zero values instead of 75% before), but we still train another regression model because we might have a better chance of understanding the relationships of the data with a lot of clear zeros already being ruled out by the first model, so the regression doesn't have to deal with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6413a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use model 1 for predicting x_train2_data, to only train the regression model on rides where a tip was predicted\n",
    "x_train2_data['Prediction'] = binary_model.predict(x_train2_data)\n",
    "x_train2_data = x_train2_data[x_train2_data['Prediction'] != 0]\n",
    "x_train2_data = x_train2_data.drop(columns=['Prediction'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed35f3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model2 = LinearRegression()\n",
    "linear_model2.fit(x_train2_data_scaled, y_train2_data)\n",
    "print(\"üìãValues for our predictors:\")\n",
    "print(str(linear_model.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339e271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin√§re Vorhersagen\n",
    "binary_predictions = binary_model.predict(x_val_data_scaled)\n",
    "\n",
    "# F√ºr Zeilen wo Tip vorhergesagt wird (binary_prediction == 1), \n",
    "# hole die kontinuierlichen Vorhersagen\n",
    "tip_amount_predictions = linear_model2.predict(x_val_data_scaled[binary_predictions == 1])\n",
    "\n",
    "# Kombiniere beide: 0 f√ºr keine Tips, vorhergesagte Werte f√ºr Tips\n",
    "final_predictions = np.zeros(len(binary_predictions))\n",
    "final_predictions[binary_predictions == 1] = tip_amount_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e226aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a graph of predicted vs actual tips\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot([y_val_data.min(), y_val_data.max()], [y_val_data.min(), y_val_data.max()], 'r--')\n",
    "plt.scatter(final_predictions[:5000], y_val_data[:5000])\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Actual Tip')\n",
    "plt.title('Actual versus predicted Tip')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691233a96f2604a1",
   "metadata": {},
   "source": [
    "<h4 style=\"text-decoration: underline;\">4. Implication/Reflection</h4>\n",
    "Let's take a look at the final perfomance of our predictive models using the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec548a60715e13e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T13:31:19.762778Z",
     "start_time": "2025-07-10T13:31:18.615766Z"
    }
   },
   "outputs": [],
   "source": [
    "linear_model_prediction = linear_model.predict(x_test_data_scaled)\n",
    "print(\"‚ùåMean absoulte Error:\", round(mean_absolute_error(y_test_data, linear_model_prediction), 4), \"\\nü§ñR¬≤:\", round(r2_score(y_test_data, linear_model_prediction), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cc44b9f3e97dfe",
   "metadata": {},
   "source": [
    "Assuming that our data was collected correctly, the results of our linear regression show that the model can explain only about 8% of the total variance in the target values (R¬≤ = 0.08). Additionally, the mean absolute error (MAE) is relatively high, which means that the predictions deviate significantly from the actual values on average. Such a low explanatory rate suggests that essential influencing factors are missing or that relationships are not correctly depicted. This means for all decision makers that the model does not make reliable predictions for any strategic or operational decisions and should therefore not be used. \n",
    "\n",
    "In order to improve the model's performance, it should be evaluated whether there are additional relevant features that could be included. This might involve incorporating other variables from the existing dataset or creating new features (e.g. polynomial features). Additionally, it would be reasonable to test non-linear modeling approaches as they might capture the real-world relationships more accurately than the current linear model (e.g. Radial Basis Function).\n",
    "\n",
    "Moreover, it could make sense to include further analyses. For example, we could review the existing features and remove those that do not significantly contribute to the prediction. To do this we could use Lasso regression which is particularly useful after implementing the improvements mentioned above if our model ends up containing a large number of features (feature selection).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

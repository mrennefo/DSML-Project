{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85dcb6d7-eabc-4f84-9b91-71c1fa1328f9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\"><h1 style=\"text-decoration: underline;\">DSML Project</h1></div>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ad31350-3590-4527-b773-f4bd3984bfa9",
   "metadata": {},
   "source": [
    "This is the official Notebook of the DSML Project from Marc Rennefort, Kilian Lipinsky, Timo Hagelberg, Jan Behrendt-Emden and Paul Severin. In order to create this Project we used the following dataset: https://data.cityofchicago.org/Transportation/Transportation-Network-Providers-Trips-2023-2024-/n26f-ihde/about_data\n",
    "<h4 style=\"text-decoration: underline;\">1. Description</h4>\n",
    "The goal of this project is to predict ride-hailing tips in Chicago based on travel time, distance, fare amount, weather conditions, and whether the customer shared the ride.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c90e750-76f6-4524-9e00-23d389391975",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:09:07.760378Z",
     "start_time": "2025-06-12T15:09:03.811946Z"
    }
   },
   "outputs": [],
   "source": [
    "#Note all your imports here\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from holoviews.plotting.plotly import ScatterPlot\n",
    "from numpy.ma.core import inner\n",
    "from pandas.core.common import random_state\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from meteostat import Hourly, Point\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb96b7050d8dc921",
   "metadata": {},
   "source": [
    "<h4 style=\"text-decoration: underline;\">2. Data Prepertion</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90df07db112fe767",
   "metadata": {},
   "source": [
    "<h4 style=\"text-decoration: underline;\">2.1 Some Basic Data Preperation</h4>\n",
    "In the first step we want to do some basic data preperartion which means that we load our data set with the columns we need, we drop all rows with null values and changing our timestamps to datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76201386-ac55-4da8-ab2b-7d5fe6a50618",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:10:33.036686Z",
     "start_time": "2025-06-12T15:09:07.778452Z"
    }
   },
   "outputs": [],
   "source": [
    "#Loading our dataset with the columns we need\n",
    "\n",
    "data_cleaned = pd.read_csv('Data/Chicago_RideHailing_Data.csv', usecols= ['Trip End Timestamp', 'Trip Seconds', 'Trip Miles', 'Fare', 'Tip', 'Trip Total','Dropoff Centroid Latitude', 'Dropoff Centroid Longitude', 'Shared Trip Authorized', 'Shared Trip Match'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28185221ab2587ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:10:39.451541Z",
     "start_time": "2025-06-12T15:10:34.141579Z"
    }
   },
   "outputs": [],
   "source": [
    "#Get some basic understanding of our data\n",
    "print('Null Values: ', data_cleaned.isnull().sum())\n",
    "data_cleaned.info()\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921cc3b-a9de-4994-83c6-e64c9cb4fb1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:10:47.648983Z",
     "start_time": "2025-06-12T15:10:39.622184Z"
    }
   },
   "outputs": [],
   "source": [
    "#Drop all rows with null values\n",
    "data_cleaned = data_cleaned.dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ba2335",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:13:39.102592Z",
     "start_time": "2025-06-12T15:10:48.304294Z"
    }
   },
   "outputs": [],
   "source": [
    "#Changing our timestamps to datetime format\n",
    "data_cleaned['Trip End Timestamp'] = pd.to_datetime(data_cleaned['Trip End Timestamp'],  format='%m/%d/%Y %I:%M:%S %p', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9270bcde-2698-40e4-9adf-315eb1f52d16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:13:41.927020Z",
     "start_time": "2025-06-12T15:13:39.115683Z"
    }
   },
   "outputs": [],
   "source": [
    "#Check if everything worked correctly\n",
    "print('Null-Werte: ', data_cleaned.isnull().sum())\n",
    "data_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8684bb9bbe0d210",
   "metadata": {},
   "source": [
    "<h4 style=\"text-decoration: underline;\">2.2 Including weather data</h4>\n",
    "In order to add the weather data we need to group our data because otherwise we will get runtime issues if we do API calls for barely 25 Million rows. This should be fine for our prediction purposes because there won't be huge differences in temperature or rain if we round by the second decimal place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f3c83a49f3c622",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:13:43.705136Z",
     "start_time": "2025-06-12T15:13:41.956727Z"
    }
   },
   "outputs": [],
   "source": [
    "#Round the Latitude and Longitude by the second decimal place and insert it in a new column\n",
    "data_cleaned[\"Latitude rounded\"] =  data_cleaned[\"Dropoff Centroid Latitude\"].round(2)\n",
    "data_cleaned[\"Longitude rounded\"] = data_cleaned[\"Dropoff Centroid Longitude\"].round(2)\n",
    "\n",
    "#Group the data by Latitude and Longitude\n",
    "data_grouped = data_cleaned.groupby([\"Latitude rounded\", \"Longitude rounded\"])[\"Trip End Timestamp\"].agg([\"min\", \"max\"]).reset_index()\n",
    "data_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e378e75201ad951f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:15:33.017544Z",
     "start_time": "2025-06-12T15:13:43.838907Z"
    }
   },
   "outputs": [],
   "source": [
    "weather_list = []\n",
    "for i in range(len(data_grouped)):\n",
    "    #Initalise variables\n",
    "    latitude = data_grouped[\"Latitude rounded\"].iloc[i]\n",
    "    longitude = data_grouped[\"Longitude rounded\"].iloc[i]\n",
    "    location = Point(latitude, longitude)\n",
    "    timestamp_min = data_grouped[\"min\"].iloc[i]\n",
    "    timestamp_max = data_grouped[\"max\"].iloc[i]\n",
    "    \n",
    "    #Round min and max column to the next hour in order to extract the weather data correctly\n",
    "    timestamp_min_rounded = timestamp_min.replace(minute = 0, second = 0) \n",
    "    timestamp_max_rounded = timestamp_max.replace(minute = 0, second = 0)\n",
    "    \n",
    "    #Extract the weather data per location\n",
    "    weather = Hourly(location, timestamp_min_rounded, timestamp_max_rounded).fetch()\n",
    "\n",
    "    #Merge the extracted weather data with the fitting timestamps and locations\n",
    "    for j in range(len(weather)):\n",
    "       weather_list.append({\"Timestamp\": weather.index[j], \"Latitude rounded\": latitude, \"Longitude rounded\": longitude, \"Temperature\": weather[\"temp\"].iloc[j], \"Rain in mm\": weather[\"prcp\"].iloc[j]})\n",
    "\n",
    "#Covert the list to a DataFrame\n",
    "weather_data = pd.DataFrame(weather_list)\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f673f12697bfe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:15:33.699636Z",
     "start_time": "2025-06-12T15:15:33.075640Z"
    }
   },
   "outputs": [],
   "source": [
    "#Now we prepare the merge of the weather data and the other data. For this we need to round our timestamps by the next hour because our weather data is given hourly\n",
    "data_cleaned[\"Trip End Timestamp Rounded\"] = data_cleaned[\"Trip End Timestamp\"].dt.floor(\"h\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84dfd98eb4e51ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:16:25.116591Z",
     "start_time": "2025-06-12T15:15:33.764228Z"
    }
   },
   "outputs": [],
   "source": [
    "#In the next step we can start with the merge\n",
    "data_merged = pd.merge(data_cleaned, weather_data, left_on=[\"Trip End Timestamp Rounded\", \"Latitude rounded\", \"Longitude rounded\"], right_on =[\"Timestamp\", \"Latitude rounded\", \"Longitude rounded\"], how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d93ac35cb9b957",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:16:28.506462Z",
     "start_time": "2025-06-12T15:16:25.352469Z"
    }
   },
   "outputs": [],
   "source": [
    "#After that we can drop all the columns we just needed to merge our data\n",
    "data_merged = data_merged.drop(columns = [\"Dropoff Centroid Latitude\", \"Dropoff Centroid Longitude\", \"Latitude rounded\", \"Longitude rounded\", \"Timestamp\" ,\"Trip End Timestamp Rounded\"])\n",
    "data_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6292de49063ad3e9",
   "metadata": {},
   "source": [
    "<h4 style=\"text-decoration: underline;\">2.3 Creation of dummy variables</h4>\n",
    "To perform or regression later on we need to transfer the 'Shared Trip Authorized', 'Shared Trip Match' and 'Trip End Timestamp' column to numeric datatype. For this we make use of dummy variable where 1 stands for true and 0 stands for false for the Shared Trip variables and different variables for the time of day in case of the Trip End variable. In addition we need to creat dummy variables for the rain because we are not interested in the amount of rain on a certain day rather we want to plot wheter it rained or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d11db014a286ddc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:16:31.463722Z",
     "start_time": "2025-06-12T15:16:30.323344Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create dummy variable for 'Shared Trip Authorized' and 'Shared Trip Match' (1 = True and 0 = False)\n",
    "data_merged[\"Shared Trip Authorized\"] = data_merged[\"Shared Trip Authorized\"].astype(int)\n",
    "data_merged[\"Shared Trip Match\"] = data_merged[\"Shared Trip Match\"].astype(int)\n",
    "print(\"üìãFirst 5 Rows:\")\n",
    "data_merged.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa41d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dummy variables for the Trip End Timestamp: 22:00-6:00 for night, 6:00-18:00 for day and 18:00-22:00 for evening\n",
    "data_merged[\"Trip End Hour\"] = data_merged[\"Trip End Timestamp\"].dt.hour\n",
    "data_merged[\"Night\"] = np.where((data_merged[\"Trip End Hour\"] >= 22) | (data_merged[\"Trip End Hour\"] < 6), 1, 0)\n",
    "data_merged[\"Day\"] = np.where((data_merged[\"Trip End Hour\"] >= 6) & (data_merged[\"Trip End Hour\"] < 18), 1, 0)\n",
    "data_merged[\"Evening\"] = np.where((data_merged[\"Trip End Hour\"] >= 18) & (data_merged[\"Trip End Hour\"] < 22), 1, 0)\n",
    "#Drop the Trip End Hour & Trip End Timestamp column as we don't need them anymore\n",
    "data_merged = data_merged.drop(columns=[\"Trip End Hour\", \"Trip End Timestamp\"])\n",
    "\n",
    "data_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0708826312343ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:16:36.030054Z",
     "start_time": "2025-06-12T15:16:32.072266Z"
    }
   },
   "outputs": [],
   "source": [
    "#We see that the column 'Rain in mm' is from datatype object but we need a numeric datatype to perform the dummy creation on this column. That's why wee need to transform this column to the right datatype\n",
    "data_merged['Rain in mm'] = pd.to_numeric(data_merged['Rain in mm'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bfab88b60b62e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:19:22.679138Z",
     "start_time": "2025-06-12T15:16:36.132797Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create dummy variables for 'Rain in mm' (0 = there was no rain, 1 = there was rain)\n",
    "rained = []\n",
    "for i in range(len(data_merged)):\n",
    "    if data_merged[\"Rain in mm\"].iloc[i] > 0:\n",
    "        rained.append(\"1\")\n",
    "    else:\n",
    "        rained.append(\"0\")\n",
    "rained_df = pd.DataFrame(rained)\n",
    "data_merged[\"Rained\"] = rained_df\n",
    "print(\"üìãFirst 5 Rows:\")\n",
    "data_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eadd0e59689eadc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:19:24.774047Z",
     "start_time": "2025-06-12T15:19:22.954350Z"
    }
   },
   "outputs": [],
   "source": [
    "#Now we need to cast the 'Rained' column to int datatype because it is from type object yet\n",
    "data_merged[\"Rained\"] = data_merged[\"Rained\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e7103215f5be0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:19:25.385267Z",
     "start_time": "2025-06-12T15:19:24.820803Z"
    }
   },
   "outputs": [],
   "source": [
    "data_merged = data_merged.drop(columns = [\"Rain in mm\"])\n",
    "print(\"üìãFirst 5 Rows:\")\n",
    "data_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c28864e3a57b2",
   "metadata": {},
   "source": [
    "<h4 style=\"text-decoration: underline;\">2.4 Dealing with outliers</h4>\n",
    "Machine Learning algorithms are highly influenced by outliers which lead to bad performance such as overfitting. That's why we have to deal with them. In the next step we want to delete all rows with values which are more than 3 standard deviations away from our mean value  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5794063011b33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:19:28.588905Z",
     "start_time": "2025-06-12T15:19:25.451116Z"
    }
   },
   "outputs": [],
   "source": [
    "#We don't need to deal with outliers on the columns with dummy variables because there can't be outliers if we just have the values 0 or 1\n",
    "data_merged_without_tip = data_merged.drop(columns = [\"Tip\", \"Shared Trip Authorized\", \"Shared Trip Match\", \"Rained\", \"Night\", \"Day\", \"Evening\"])\n",
    "for columns in data_merged_without_tip.columns:\n",
    "    #Calculate mean and standard deviation of the current column\n",
    "    mean = data_merged_without_tip[columns].mean()\n",
    "    std = data_merged_without_tip[columns].std()\n",
    "    \n",
    "    #Calculate upper and lower limit\n",
    "    upperlimit = mean + 3 * std\n",
    "    lowerlimit = mean - 3 * std\n",
    "\n",
    "    #Replace all outliers with null values so we can remove them later\n",
    "    data_merged.loc[(data_merged[columns] > upperlimit) | (data_merged[columns] < lowerlimit), columns] = np.nan\n",
    "\n",
    "#Remove all null values (delete all outliers)    \n",
    "data_without_outliers = data_merged.dropna(axis = 0)\n",
    "print(f\"‚ùå Deleted {len(data_merged) - len(data_without_outliers)} outliers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e10fae",
   "metadata": {},
   "source": [
    "<h4 style=\"text-decoration: underline;\">2.5 Saving the data</h4>\n",
    "We will save the preperated data here, so that we can continue working with it, without having to rerun all the code above every time we start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf3b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_outliers.to_csv('Data/Chicago_RideHailing_Data_Cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4010aeb95861dbd",
   "metadata": {},
   "source": [
    "<h4 style=\"text-decoration: underline;\">3. Data Modeling</h4>\n",
    "First, we can load the saved data, if we didn't rerun the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30556712",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_outliers = pd.read_csv('Data/Chicago_RideHailing_Data_Cleaned.csv')\n",
    "#Check if everything worked correctly\n",
    "data_without_outliers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90765fe319b7467",
   "metadata": {},
   "source": [
    "First of all we need to split our data in train, test and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982ea1825479e2d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:20:10.052235Z",
     "start_time": "2025-06-12T15:19:29.257951Z"
    }
   },
   "outputs": [],
   "source": [
    "#Define x and y vectors\n",
    "x = data_without_outliers.drop(columns = [\"Tip\"])\n",
    "y = data_without_outliers[\"Tip\"]\n",
    "\n",
    "#Perform train test validation split\n",
    "x_train_data, x_test_data, y_train_data, y_test_data = train_test_split(x, y, test_size = 0.3, random_state = 42)\n",
    "x_val_data, x_test_data, y_val_data, y_test_data = train_test_split(x_test_data, y_test_data, test_size = 0.5, random_state = 42)\n",
    "print(\"Datasplit:\")\n",
    "print(f\"üèãÔ∏è Training: {len(x_train_data)} Samples ({len(x_train_data)/len(data_without_outliers)*100:.1f}%)\")\n",
    "print(f\"üî¨ Testing: {len(x_test_data)} Samples ({len(x_test_data)/len(data_without_outliers)*100:.1f}%)\")\n",
    "print(f\"‚úÖ Validation: {len(x_val_data)} Samples ({len(x_val_data)/len(data_without_outliers)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e6f7fdb19fb143",
   "metadata": {},
   "source": [
    "Let's plot the data to get an first overview on our predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e774fd7e0d771cb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:24:52.086304Z",
     "start_time": "2025-06-12T15:20:11.917071Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create Scatterplots for the first 4 predictors\n",
    "fig_1, axes_1 = plt.subplots(nrows = 1, ncols = 4, figsize= (21,6))\n",
    "fig_1.suptitle(\"Scatterplots of all predictors\", fontsize=26)\n",
    "for i, ax in enumerate(axes_1):\n",
    "    ax.scatter(x = x_train_data.iloc[:, i], y = y_train_data, color = f'C{i}')\n",
    "    ax.set_title(x_train_data.columns[i])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Create Scatterplots for predictor 5-8\n",
    "fig_2, axes_2 = plt.subplots(nrows = 1, ncols = 4, figsize= (21,6))\n",
    "for i, ax in enumerate(axes_2):\n",
    "    ax.scatter(x = x_train_data.iloc[:, i + 4], y = y_train_data, color = f'C{i + 4}')\n",
    "    ax.set_title(x_train_data.columns[i + 4])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a560e5dc95786d",
   "metadata": {},
   "source": [
    "After plotting the data we get some interesting information. It looks like there are a lot of linear correlations for example between the trip total and the tip. Moreover it is noticeable that the predictors are on diffrent scales so we need to normalize them to get meaningful result in our regression later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227744ba4d1020f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:24:56.261648Z",
     "start_time": "2025-06-12T15:24:52.903172Z"
    }
   },
   "outputs": [],
   "source": [
    "#Normalize data\n",
    "scaler = StandardScaler()\n",
    "x_train_data_scaled = scaler.fit_transform(x_train_data)\n",
    "#Hier nur noch transform verwenden, damit Mittelwert und Standardabweichung nicht neu berechnet werden\n",
    "x_val_data_scaled = scaler.transform(x_val_data)\n",
    "x_test_data_scaled = scaler.transform(x_test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
